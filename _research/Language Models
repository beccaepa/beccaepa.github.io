---
title: Language Models Learn to Mislead Humans via RLHF

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/x1.png

# This is optional, if not provided it will not show on the page.
subtitle: arXiv 2024

# This is optional, if not provided the title will not have a link to anywhere
link: https://arxiv.org/pdf/2409.12822

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Jiaxin Wen
  - name: Ruiqi Zhong
  - name: Akbir Khan
  - name: Ethan Perez
  - name: Jacob Steinhardt
  - name: Minlie Huang
  - name: Samuel R. Boman
  - name: He He, Shi Feng

# List of links
links:
---

<!--Abstract-->
Language models (LMs) can produce hard-to-detect errors, especially in complex tasks, and we study this phenomenon under a standard RLHF pipeline, calling it “U-Sophistry” since it is Unintended by model developers, where time-constrained human subjects evaluate model outputs, revealing that RLHF improves LMs' ability to convince but not their accuracy, and shows that existing detection methods do not generalize to U-Sophistry, highlighting a significant failure mode of RLHF and the need for more research in human alignment.
