---
title: Training Language Models with Language Feedback at Scale

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/training_LMs_image.png

# This is optional, if not provided it will not show on the page.
subtitle: arXiv 2023

# This is optional, if not provided the title will not have a link to anywhere
link: https://arxiv.org/pdf/2303.16755.pdf

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Jérémy Scheurer
  - name: Jon Ander Campos
  - name: Tomasz Korbak
  - name: Jun Shern Chan
  - name: Angelica Chen
  - name: Kyunghyun Cho
  - name: Ethan Perez

links:
  - name: Blog Post
    url: https://www.lesswrong.com/posts/mCZSXdZoNoWn5SkvE/imitation-learning-from-language-feedback-1
  - name: Code
    url: https://github.com/JeremyAlain/imitation_learning_from_language_feedback
  - name: FAR AI
    url: https://far.ai/publication/scheurer2023training/
  - name: Twitter Thread
    url: https://twitter.com/jeremy_scheurer/status/1668292694563758080
---

<!--Abstract-->

Pretrained language models often generate harmful or incorrect outputs. Imitation Learning from Language Feedback addresses this issue leading to roughly human-level summarization performance.
