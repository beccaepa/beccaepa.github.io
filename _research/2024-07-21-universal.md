---
title: When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/universal-image.png

# This is optional, if not provided it will not show on the page.
subtitle: arXiv 2024

# This is optional, if not provided the title will not have a link to anywhere
link: https://arxiv.org/pdf/2407.15211

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Rylan Schaeffer
  - name: Dan Valentine
  - name: Luke Bailey
  - name: James Chua
  - name: Crist√≥bal Eyzaguirre
  - name: Zane Durante
  - name: Joe Benton
  - name: Brando Miranda
  - name: Henry Sleight
  - name: John Hughes
  - name: Rajashree Agrawal
  - name: Mrinank Sharma
  - name: Scott Emmons
  - name: Sanmi Koyejo
  - name: Ethan Perez

links:
  - name: Code
    url: https://github.com/RylanSchaeffer/AstraFellowship-When-Do-VLM-Image-Jailbreaks-Transfer
  - name: Twitter Thread
    url: https://x.com/RylanSchaeffer/status/1818308718330630194

---

<!--Abstract-->

In this work, we focus on a popular class of vision-language models (VLMs) that generate text outputs conditioned on visual and textual inputs.
