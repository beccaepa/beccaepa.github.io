---
title: "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/red_reduce_harms_image.png

# This is optional, if not provided it will not show on the page.
subtitle: arXiv 2022

# This is optional, if not provided the title will not have a link to anywhere
link: https://arxiv.org/pdf/2209.07858.pdf

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Deep Ganguli
  - name: Liane Lovitt
  - name: Jackson Kernion
  - name: Amanda Askell
  - name: Yuntao Bai
  - name: Saurav Kadavath
  - name: Ben Mann
  - name: Ethan Perez
  - name: Nicholas Schiefer
  - name: Kamal Ndousse
  - name: Andy Jones
  - name: Sam Bowman
  - name: Anna Chen
  - name: Tom Conerly
  - name: Nova DasSarma
  - name: Dawn Drain
  - name: Nelson Elhage
  - name: Sheer El-Showk
  - name: Stanislav Fort
  - name: Zac Hatfield Dodds
  - name: Tom Henighan
  - name: Danny Hernandez
  - name: Tristan Hume
  - name: Josh Jacobson
  - name: Scott Johnston 
  - name: Shauna Kravec
  - name: Catherine Olsson
  - name: Sam Ringer
  - name: Eli Tran-Johnson
  - name: Dario Amodei
  - name: Tom Brown
  - name: Nicholas Joseph
  - name: Sam McCandlish
  - name: Chris Olah
  - name: Jared Kaplan
  - name: Jack Clark

links:
  - name: Code
    url: https://github.com/anthropics/hh-rlhf
  - name: Twitter Thread
    url: https://twitter.com/AnthropicAI/status/1571988929800273932?lang=en
---

<!--Abstract-->

We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs.
