---
title: "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training"

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/sleeper-agents-image.png

# This is optional, if not provided it will not show on the page.
subtitle: arXiv 2024

# This is optional, if not provided the title will not have a link to anywhere
link: https://arxiv.org/pdf/2401.05566.pdf

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Evan Hubinger
  - name: Carson Denison
  - name: Jesse Mu
  - name: Mike Lambert
  - name: Meg Tong
  - name: Monte MacDiarmid
  - name: Tamera Lanham
  - name: Daniel M Ziegler
  - name: Tim Maxwell
  - name: Newton Cheng
  - name: Adam Jermyn
  - name: Amanda Askell
  - name: Ansh Radhakrishnan
  - name: Cem Anil
  - name: David Duvenaud
  - name: Deep Ganguli
  - name: Fazl Barez
  - name: Jack Clark
  - name: Kamal Ndousse
  - name: Kshitij Sachan
  - name: Michael Sellitto
  - name: Mrinank Sharma
  - name: Nova DasSarma
  - name: Roger Grosse
  - name: Shauna Kravec
  - name: Yuntao Bai
  - name: Zachary Witten
  - name: Marina Favaro
  - name: Jan Brauner
  - name: Holden Karnofsky
  - name: Paul Christiano
  - name: Samuel R Bowman
  - name: Logan Graham
  - name: Jared Kaplan
  - name: SÃ¶ren Mindermann
  - name: Ryan Greenblatt
  - name: Buck Shlegeris
  - name: Nicholas Schiefer
  - name: **Ethan Perez**

---

<!--Abstract-->

Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? 
