---
title: Many-shot Jailbreaking

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/many-shot-image.png

# This is optional, if not provided it will not show on the page.
subtitle: 

# This is optional, if not provided the title will not have a link to anywhere
link: https://www-cdn.anthropic.com/af5633c94ed2beb282f6a53c595eb437e8e7b630/Many%5C_Shot%5C_Jailbreaking%5C_%5C_2024%5C_04%5C_02%5C_0936.pdf

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Cem Anil
  - name: Esin Durmus
  - name: Mrinank Sharma
  - name: Joe Benton
  - name: Sandipan Kundu
  - name: Joshua Batson
  - name: Nina Rimsky
  - name: Meg Tong
  - name: Jesse Mu
  - name: Daniel Ford
  - name: Francesco Mosconi
  - name: Rajashree Agrawal
  - name: Rylan Schaeffer
  - name: Naomi Bashkansky
  - name: Samuel Svenningsen
  - name: Mike Lambert
  - name: Ansh Radhakrishnan
  - name: Carson Denison
  - name: Evan J Hubinger
  - name: Yuntao Bai
  - name: Trenton Bricken
  - name: Timothy Maxwell
  - name: Nicholas Schiefer
  - name: Jamie Sully
  - name: Alex Tamkin
  - name: Tamera Lanham
  - name: Karina Nguyen
  - name: Tomasz Korbak
  - name: Jared Kaplan
  - name: Deep Ganguli
  - name: Samuel R. Bowman
  - name: Ethan Perez*
  - name: Roger Grosse*
  - name: David Duvenaud*

links:
  - name: Twitter Thread
    url: https://x.com/AnthropicAI/status/1775211248239464837

---

<!--Abstract-->

We investigate a family of simple long-context attacks on large language models: prompting with hundreds of demonstrations of undesirable behavior. 
