---
title: Pretraining Language Models with Human Preferences

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/pretraining_LMs_image.png

# This is optional, if not provided it will not show on the page.
subtitle: ICML 2023

# This is optional, if not provided the title will not have a link to anywhere
link: https://proceedings.mlr.press/v202/korbak23a/korbak23a.pdf

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Tomasz Korbak
  - name: Kejian Shi
  - name: Angelica Chen
  - name: Rasika Bhalerao
  - name: Christopher L. Buckley
  - name: Jason Phang
  - name: Samuel R. Bowman
  - name: Ethan Perez

links:
  - name: Blog Post
    url: https://www.lesswrong.com/posts/8F4dXYriqbsom46x5/pretraining-language-models-with-human-preferences
  - name: Code
    url: https://github.com/tomekkorbak/pretraining-with-human-feedback
  - name: FAR AI
    url: https://far.ai/publication/korbak2023pretraining/
  - name: Talk
    url: https://youtu.be/r8ibxJEleVI?si=ShCnTj8RI12zzaSi
---

<!--Abstract-->

We propose methods for pretraining language models with human preferences, resulting in much better preference satisfaction than standard pretraining-then-finetune paradigm.
