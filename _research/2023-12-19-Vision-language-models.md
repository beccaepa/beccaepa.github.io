---
title: Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/visual-image.png

# This is optional, if not provided it will not show on the page.
subtitle: NeurIPS 2023 Workshop

# This is optional, if not provided the title will not have a link to anywhere
link: https://arxiv.org/pdf/2310.12921.pdf

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Juan Rocamonde
  - name: Victoriano Montesinos
  - name: Elvis Nava
  - name: Ethan Perez* 
  - name: David Lindner*

links:
  - name: Code
    url: https://github.com/AlignmentResearch/vlmrm
  - name: FAR AI
    url: https://far.ai/post/2023-10-vlm-rm/
  - name: Twitter Thread
    url: https://twitter.com/EthanJPerez/status/1716523528353382411
  - name: Website
    url: https://sites.google.com/view/vlm-rm
---

<!--Abstract-->

We study a more sample-efficient alternative than reinforcement learning (RL): using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. 
