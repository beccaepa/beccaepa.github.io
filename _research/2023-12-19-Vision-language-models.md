---
title: Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/visual-image.png

# This is optional, if not provided it will not show on the page.
subtitle: arXiv 2023

# This is optional, if not provided the title will not have a link to anywhere
link: https://arxiv.org/pdf/2310.12921.pdf

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Juan Rocamonde
  - name: Victoriano Montesinos
  - name: Elvis Nava
  - name: Ethan Perez
  - name: David Lindner

---

<!--Abstract-->

We study a more sample-efficient alternative than reinforcement learning (RL): using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. 
