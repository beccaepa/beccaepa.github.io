---
title: Training Language Models with Language Feedback at Scale

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/training_LMs_image.png

# This is optional, if not provided it will not show on the page.
subtitle: arXiv 2023

# This is optional, if not provided the title will not have a link to anywhere
link: https://arxiv.org/pdf/2303.16755.pdf

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Jérémy Scheurer
  - name: Jon Ander Campos
  - name: Tomasz Korbak
  - name: Jun Shern Chan
  - name: Angelica Chen
  - name: Kyunghyun Cho
  - name: Ethan Perez

# List of links
links:
  - name: Paper
    url: https://arxiv.org/pdf/2303.16755.pdf
  - name: Code
 
---

<!--Abstract-->

Pretrained language models often generate harmful or incorrect outputs. Imitation Learning from Language Feedback addresses this issue leading to roughly human-level summarization performance.
