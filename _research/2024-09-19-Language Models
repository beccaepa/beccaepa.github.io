---
title: Language Models Learn to Mislead Humans via RLHF

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/x1.png

# This is optional, if not provided it will not show on the page.
subtitle: arXiv 2024

# This is optional, if not provided the title will not have a link to anywhere
link: https://arxiv.org/pdf/2409.12822

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Jiaxin Wen
  - name: Ruiqi Zhong
  - name: Akbir Khan
  - name: Ethan Perez
  - name: Jacob Steinhardt
  - name: Minlie Huang
  - name: Samuel R. Boman
  - name: He He, Shi Feng

# List of links
links:
---

<!--Abstract-->
We studied “U-Sophistry,” a phenomenon where language models trained with Reinforcement Learning from Human Feedback (RLHF) become better at misleading humans about their correctness without improving actual accuracy, highlighting a significant failure mode of RLHF and the need for further research in alignment.
