---
title: RL with KL Penalties is Better Viewed as Bayesian Inference

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/rl_with_kl_image.png

# This is optional, if not provided it will not show on the page.
subtitle: EMNLP 2022

# This is optional, if not provided the title will not have a link to anywhere
link: https://arxiv.org/pdf/2205.11275.pdf

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Tomasz Korbak
  - name: Ethan Perez
  - name: Christopher L Buckley

# List of links
links:
  - name: Blog Post
    url: https://www.alignmentforum.org/posts/eoHbneGvqDu25Hasc/rl-with-kl-penalties-is-better-seen-as-bayesian-inference
  - name: FAR AI
    url: https://far.ai/publication/korbak2022rl/#:~:text=We%20argue%20that%20this%20Bayesian,principles%20derivation%20for%20its%20objective.
  - name: Twitter Thread
    url: https://twitter.com/tomekkorbak/status/1594762112748126208
---

<!--Abstract-->

KL penalties in RL with language models arenâ€™t a hack; KL penalties have a principled, Bayesian justification.
