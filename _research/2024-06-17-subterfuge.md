---
title: "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models"

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/subterfuge-image.png

# This is optional, if not provided it will not show on the page.
subtitle: arXiv 2024

# This is optional, if not provided the title will not have a link to anywhere
link: https://arxiv.org/pdf/2406.10162

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Carson Denison*
  - name: Monte MacDiarmid
  - name: Fazl Barez
  - name: David Duvenaud
  - name: Shauna Kravec
  - name: Samuel Marks
  - name: Nicholas Schiefer
  - name: Ryan Soklaski
  - name: Alex Tamkin
  - name: Jared Kaplan
  - name: Buck Shlegeris
  - name: Samuel R. Bowman
  - name: Ethan Perez
  - name: Evan Hubinger*

# List of links
links:
  - name: Blog Post
    url: https://www.alignmentforum.org/posts/FSgGBjDiaCdWxNBhj/sycophancy-to-subterfuge-investigating-reward-tampering-in
  - name: Code
    url: https://github.com/anthropics/sycophancy-to-subterfuge-paper
  - name: Twitter Thread
    url: https://x.com/AnthropicAI/status/1802743256461046007
---

<!--Abstract-->

In this paper, we study whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering.
