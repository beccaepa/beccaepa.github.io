---
title: Sabotage Evaluations for Frontier Models

# Path to the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
image: /assets/images/sabotage.jpeg

# This is optional, if not provided it will not show on the page.
subtitle: arXiv 2024

# This is optional, if not provided the title will not have a link to anywhere
link: https://arxiv.org/pdf/2410.21514

# Add list of authors here.
# Name is mandatory, url is optional.
authors:
  - name: Joe Benton
  - name: Misha Wagner
  - name: Eric Christiansen
  - name: Cem Anil
  - name: Ethan Perez
  - name: Jai Srivastav
  - name: Esin Durmus
  - name: Deep Ganguli
  - name: Shauna Kravec
  - name: Buck Shlegeris
  - name: Jared Kaplan
  - name: Holden Karnofsky
  - name: Evan Hubinger
  - name: Roger Grosse
  - name: Samuel R Bowman
  - name: David Duvenaud

# List of links
links:
  - name: Blog Post
    url: https://www.anthropic.com/research/sabotage-evaluations
  - name: Twitter Thread
    url: https://x.com/AnthropicAI/status/1847335821113782379
---

<!--Abstract-->

This paper examines the risk of AI models developing "sabotage capabilities" that could undermine human oversight in critical contexts like AI development and deployment.
