---
layout: home
image: /assets/images/EthanPerezSquareHeadshot2019-2.jpg
---

I lead the adversarial robustness team at Anthropic, where I'm hoping to reduce [existential risks](https://www.safe.ai/statement-on-ai-risk) from AI systems. I also spend some time at New York University (NYU) collaborating with Sam Bowmanâ€™s AI safety research group.

I received my PhD from NYU under the supervision of Kyunghyun Cho and Douwe Kiela and funded by NSF and Open Philanthropy. Previously, I've spent time at DeepMind, Facebook AI Research, Montreal Institute for Learning Algorithms, and Google.

I helped to develop [Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2005.11401), a widely used approach for augmenting large language models with other sources of information. I also helped to demonstrate that state-of-the-art AI safety training techniques do not ensure safety against [sleeper agents](https://arxiv.org/abs/2401.05566).

[Email](mailto:perez@nyu.edu) /
[Google Scholar](https://scholar.google.ca/citations?user=za0-taQAAAAJ&hl=en) /
[GitHub](https://github.com/ethanjperez) /
[Twitter](https://twitter.com/EthanJPerez) /
[CV](/assets/pdfs/CV.pdf)
