---
# Always set this as 'post'.
layout: post

# Set the title of the blog here
title:  "Inverse Scaling Prize Ideas"

# Set the date here, format must be YYYY-MM-DD hh:mm:ss +TTTT.
date: 2022-10-9 00:00:00 +0000

# Set the path of the image file, example /assets/images/blog/image.jpg
# You can also use an online URL as well, example https://www.google.com/image.jpg
# Image is optional, if not provided no image will be shown.
image: /assets/images/inverse-scaling-prize-ideas.png

# This is an optional field which sets the link of the blog post.
# If not provided, a link will be generated automatically with the title of the blog post.
permalink: blog/inverse-scaling-prize-ideas_DUMMY/

# This is the excerpt of the blog post which will be shown in the blog listing page.
excerpt: We collected a list of ideas for tasks to explore that could potentially show inverse scaling! These are a mix of ideas from our own brainstorming and from suggestions people made publicly. We have included links to the original suggestions where applicable.
---

<!-- Add the blog post here in markdown -->

![Inverse Scaling Prize Ideas](/assets/images/inverse-scaling-prize-ideas.png)

*Written by [Alex Lyzhov](https://www.linkedin.com/in/alexlyzhov/), [Ian McKenzie](https://irmckenzie.co.uk/), and [Ethan Perez](https://ethanperez.net/)*

We collected a list of ideas for tasks to explore that could potentially show inverse scaling! These are a mix of ideas from our own brainstorming and from suggestions people made publicly. We have included zlinks to the original suggestions where applicable.

(Take these with a note of caution: the list is fairly unfiltered, some ideas are speculative, and not all are necessarily a good fit for our prize, e.g. if they involve finetuning an LM.)

1. Are larger language models (LMs) more stubborn in conversations? Suppose a large LM generates some flawed text (e.g., factually incorrect or socially-biased text). Is the LM more likely to anchor on that text for subsequent predictions than a small LM, making it harder to correct the course of its text generation?  
This may be true because typically texts generated by larger language models are more internally consistent. This may extend to undesirable consistency, e.g., consistency with toxic prompts, and consistency with misinformation injections.
